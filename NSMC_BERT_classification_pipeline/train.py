from dataset import NSMC_Dataset
from trainer import return_training_arguments
from metrics import compute_metrics, sentences_predict
from transformers import BertForSequenceClassification, Trainer, TrainingArguments

import torch

def main():
    train_dataset = NSMC_Dataset('train')
    test_dataset = NSMC_Dataset('test')

    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

    training_args = return_training_arguments()

    MODEL_NAME = "bert-base-multilingual-cased"
    model = BertForSequenceClassification.from_pretrained(MODEL_NAME)
    model.to(device)

    train_trainer = Trainer(
    model=model,                         # the instantiated ğŸ¤— Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    )
    # print('í•™ìŠµì‹œì‘.. 1 epochì— ëŒ€ëµ 30ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤ :-)')
    # train_trainer.train() # 1 epochì— ëŒ€ëµ 30ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤ :-)


    eval_trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics
    )
    # print('evaluate ì‹œì‘..' )
    # eval_trainer.evaluate(eval_dataset=test_dataset)


    pred_param = [model,train_dataset.tokenizer,device]
    print(sentences_predict(["ì˜í™” ê°œì¬ë°Œì–´ ã…‹ã…‹ã…‹ã…‹ã…‹"]+pred_param))
    # print(sentences_predict("ì§„ì§œ ì¬ë¯¸ì—†ë„¤ìš” ã…‹ã…‹"))
    # print(sentences_predict("ë„ˆ ë•Œë¬¸ì— ì§„ì§œ ì§œì¦ë‚˜"))
    # print(sentences_predict("ì •ë§ ì¬ë°Œê³  ì¢‹ì•˜ì–´ìš”."))


    
    
if __name__ == '__main__':
    main()